{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcdea072c5d35630",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": "<a href=\"https://colab.research.google.com/github/fengge/agentic-rag-for-dummies/blob/main/Agentic_Rag_For_Dummies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "id": "e0fd1790",
   "metadata": {
    "id": "e0fd1790"
   },
   "source": [
    "# Agentic RAG for Dummies\n",
    "\n",
    "An advanced Retrieval-Augmented Generation (RAG) system that uses intelligent agents to retrieve and synthesize information from PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d06c15",
   "metadata": {
    "id": "c7d06c15"
   },
   "source": [
    "## 1. Dependencies Installation\n",
    "\n",
    "Install required packages for the RAG system.\n",
    "\n",
    "**Documentation:**\n",
    "- [LangGraph](https://langchain-ai.github.io/langgraph/) - Framework for building multi-agent applications\n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction) - Framework for developing LLM applications\n",
    "- [Qdrant](https://qdrant.tech/documentation/) - Vector database for similarity search\n",
    "- [Gradio](https://www.gradio.app/docs) - Web interface for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321dbfd3",
   "metadata": {
    "id": "321dbfd3",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#run this cell only in colab, otherwise create a venv and install requirements.txt available in the project folder\n",
    "!pip install --quiet --upgrade langgraph\n",
    "!pip install -qU langchain-ollama\n",
    "!pip install -qU langchain langchain-community langchain-qdrant langchain-huggingface qdrant-client fastembed flashrank langchain-core\n",
    "!pip install --upgrade gradio\n",
    "\n",
    "# Optional (example): if you want to use Gemini models\n",
    "#!pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c3e2d",
   "metadata": {
    "id": "fd7c3e2d"
   },
   "source": [
    "## 2. Environment Configuration\n",
    "\n",
    "Set up directory structure and environment variables for document processing.\n",
    "\n",
    "**What it does:**\n",
    "- Creates directories for storing PDFs, Markdown files, and parent chunks\n",
    "- Defines collection names for the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9782958",
   "metadata": {
    "id": "c9782958",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configuration\n",
    "DOCS_DIR = \"docs\"  # Directory containing your pdfs files\n",
    "MARKDOWN_DIR = \"markdown\" # Directory containing the pdfs converted to markdown\n",
    "PARENT_STORE_PATH = \"parent_store\"  # Directory for parent chunk JSON files\n",
    "CHILD_COLLECTION = \"document_child_chunks\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "os.makedirs(MARKDOWN_DIR, exist_ok=True)\n",
    "os.makedirs(PARENT_STORE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d855e643bcf0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06eca5f1",
   "metadata": {
    "id": "06eca5f1"
   },
   "source": [
    "## 3. LLM Initialization\n",
    "\n",
    "Initialize the Large Language Model that will power the conversational agent.\n",
    "\n",
    "**What it does:**\n",
    "- Configures the LLM using Ollama (local inference)\n",
    "- Alternative example provided for Google Gemini\n",
    "\n",
    "**Documentation:**\n",
    "- [LangChain Ollama](https://python.langchain.com/docs/integrations/chat/ollama)\n",
    "- [LangChain Google GenAI](https://python.langchain.com/docs/integrations/chat/google_generative_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35baef71-69a0-4186-b7c3-d31735152d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Environment Bootstrap =====\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# quick sanity check (å¯é€‰)\n",
    "assert os.getenv(\"SILICONFLOW_API_KEY\") is not None, \"SILICONFLOW_API_KEY not found in .env\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08020f-ebff-4e16-987f-0bc9b1e64e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install -U langchain-openai python-dotenv\n",
    "print(\"KEY loaded:\", bool(os.getenv(\"SILICONFLOW_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8989b7",
   "metadata": {
    "id": "7e8989b7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# å»ºè®®ï¼šæŠŠ KEY æ”¾åˆ°çŽ¯å¢ƒå˜é‡é‡Œï¼ˆWindows PowerShell: $env:SILICONFLOW_API_KEY=\"xxx\"ï¼‰\n",
    "SILICONFLOW_API_KEY = os.getenv(\"SILICONFLOW_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=SILICONFLOW_API_KEY,\n",
    "    base_url=\"https://api.siliconflow.cn/v1\",\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\",   # ä½ ä¹Ÿå¯ä»¥æ¢æˆä½ åœ¨ç¡…åŸºæµåŠ¨æŽ§åˆ¶å°çœ‹åˆ°çš„æ¨¡åž‹å\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058f292",
   "metadata": {
    "id": "5058f292"
   },
   "source": [
    "## 4. Embeddings Setup\n",
    "\n",
    "Configure embedding models for semantic search using hybrid retrieval (dense + sparse).\n",
    "\n",
    "**What it does:**\n",
    "- **Dense embeddings**: Capture semantic meaning using neural networks\n",
    "- **Sparse embeddings**: Provide keyword-based matching (BM25 algorithm)\n",
    "\n",
    "**Documentation:**\n",
    "- [HuggingFace Embeddings](https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub)\n",
    "- [FastEmbed Sparse](https://qdrant.github.io/fastembed/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f982c53",
   "metadata": {
    "id": "9f982c53"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
    "\n",
    "# Dense embeddings for semantic understanding\n",
    "dense_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Sparse embeddings for keyword matching\n",
    "sparse_embeddings = FastEmbedSparse(\n",
    "    model_name=\"Qdrant/bm25\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc968a4c",
   "metadata": {
    "id": "bc968a4c"
   },
   "source": [
    "## 5. Vector Database Configuration\n",
    "\n",
    "Set up Qdrant vector database for storing and retrieving document embeddings.\n",
    "\n",
    "**What it does:**\n",
    "- Initializes local Qdrant client with file-based storage\n",
    "- Creates collections with both dense and sparse vector configurations\n",
    "- Enables hybrid search capabilities\n",
    "\n",
    "**Documentation:**\n",
    "- [LangChain Qdrant](https://python.langchain.com/docs/integrations/vectorstores/qdrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efc63b",
   "metadata": {
    "id": "11efc63b"
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_qdrant.qdrant import RetrievalMode\n",
    "\n",
    "# Initialize Qdrant client (local file-based storage)\n",
    "client = QdrantClient(path=\"qdrant_db\")\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
    "\n",
    "def ensure_collection(collection_name):\n",
    "    \"\"\"Create Qdrant collection if it doesn't exist\"\"\"\n",
    "    if not client.collection_exists(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=qmodels.VectorParams(\n",
    "                size=embedding_dimension,\n",
    "                distance=qmodels.Distance.COSINE\n",
    "            ),\n",
    "            sparse_vectors_config={\n",
    "                \"sparse\": qmodels.SparseVectorParams()\n",
    "            },\n",
    "        )\n",
    "        print(f\"âœ“ Created collection: {collection_name}\")\n",
    "    else:\n",
    "        print(f\"âœ“ Collection already exists: {collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec851a6",
   "metadata": {
    "id": "bec851a6"
   },
   "source": [
    "## 6. PDF to Markdown Conversion\n",
    "\n",
    "Convert PDF documents to Markdown format for better text extraction and processing.\n",
    "\n",
    "**What it does:**\n",
    "- Uses PyMuPDF to extract text from PDFs\n",
    "- Converts documents to clean Markdown format\n",
    "- Handles encoding issues and removes images\n",
    "- Skips already converted files unless overwrite is enabled\n",
    "\n",
    "**Note:** For more details on PDF conversion, refer to the `pdf_to_md.ipynb` notebook in the repository.\n",
    "\n",
    "**Documentation:**\n",
    "- [PyMuPDF](https://pymupdf.readthedocs.io/)\n",
    "- [PyMuPDF4LLM](https://github.com/pymupdf/PyMuPDF4LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd6518",
   "metadata": {
    "id": "06fd6518"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf.layout\n",
    "import pymupdf4llm\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def pdf_to_markdown(pdf_path, output_dir):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    md = pymupdf4llm.to_markdown(doc, header=False, footer=False, page_separators=True, ignore_images=True, write_images=False, image_path=None)\n",
    "    md_cleaned = md.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='ignore')\n",
    "    output_path = Path(output_dir) / Path(doc.name).stem\n",
    "    Path(output_path).with_suffix(\".md\").write_bytes(md_cleaned.encode('utf-8'))\n",
    "\n",
    "def pdfs_to_markdowns(path_pattern, overwrite: bool = False):\n",
    "    output_dir = Path(MARKDOWN_DIR)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pdf_path in map(Path, glob.glob(path_pattern)):\n",
    "        md_path = (output_dir / pdf_path.stem).with_suffix(\".md\")\n",
    "        if overwrite or not md_path.exists():\n",
    "            pdf_to_markdown(pdf_path, output_dir)\n",
    "\n",
    "pdfs_to_markdowns(f\"{DOCS_DIR}/*.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfff03",
   "metadata": {
    "id": "e6bfff03"
   },
   "source": [
    "## 7. Document Indexing\n",
    "\n",
    "Implement parent-child chunking strategy for optimal retrieval performance.\n",
    "\n",
    "**What it does:**\n",
    "- Splits documents into hierarchical chunks (parent and child)\n",
    "- **Parent chunks**: Large context windows (2000-10000 chars) stored as JSON\n",
    "- **Child chunks**: Small searchable units (500 chars) stored in Qdrant\n",
    "- Merges small chunks and splits large ones for consistency\n",
    "- Creates bidirectional links between parent and child chunks\n",
    "\n",
    "**Chunking Strategy:**\n",
    "1. Split by Markdown headers (#, ##, ###)\n",
    "2. Merge chunks smaller than 2000 characters\n",
    "3. Split chunks larger than 10000 characters\n",
    "4. Create child chunks (500 chars) from each parent\n",
    "5. Store parent chunks in JSON files\n",
    "6. Index child chunks in vector database\n",
    "\n",
    "**Documentation:**\n",
    "- [LangChain Text Splitters](https://docs.langchain.com/oss/python/integrations/splitters)\n",
    "- [LangChain Split Markdown](https://docs.langchain.com/oss/python/integrations/splitters/markdown_header_metadata_splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103d352",
   "metadata": {
    "id": "e103d352"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "if client.collection_exists(CHILD_COLLECTION):\n",
    "    print(f\"Removing existing Qdrant collection: {CHILD_COLLECTION}\")\n",
    "    client.delete_collection(CHILD_COLLECTION)\n",
    "    ensure_collection(CHILD_COLLECTION)\n",
    "else:\n",
    "    ensure_collection(CHILD_COLLECTION)\n",
    "\n",
    "child_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=CHILD_COLLECTION,\n",
    "    embedding=dense_embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    sparse_vector_name=\"sparse\"\n",
    ")\n",
    "\n",
    "def index_documents():\n",
    "    headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
    "    parent_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "    min_parent_size = 2000\n",
    "    max_parent_size = 10000\n",
    "\n",
    "    all_parent_pairs, all_child_chunks = [], []\n",
    "    md_files = sorted(glob.glob(os.path.join(MARKDOWN_DIR, \"*.md\")))\n",
    "\n",
    "    if not md_files:\n",
    "        print(f\"âš ï¸  No .md files found in {MARKDOWN_DIR}/\")\n",
    "        return\n",
    "\n",
    "    for doc_path_str in md_files:\n",
    "        doc_path = Path(doc_path_str)\n",
    "        print(f\"ðŸ“„ Processing: {doc_path.name}\")\n",
    "\n",
    "        try:\n",
    "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                md_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading {doc_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        parent_chunks = parent_splitter.split_text(md_text)\n",
    "        merged_parents = merge_small_parents(parent_chunks, min_parent_size)\n",
    "        split_parents = split_large_parents(merged_parents, max_parent_size, child_splitter)\n",
    "        cleaned_parents = clean_small_chunks(split_parents, min_parent_size)\n",
    "\n",
    "        for i, p_chunk in enumerate(cleaned_parents):\n",
    "            parent_id = f\"{doc_path.stem}_parent_{i}\"\n",
    "            p_chunk.metadata.update({\"source\": doc_path.stem + \".pdf\", \"parent_id\": parent_id})\n",
    "            all_parent_pairs.append((parent_id, p_chunk))\n",
    "            children = child_splitter.split_documents([p_chunk])\n",
    "            all_child_chunks.extend(children)\n",
    "\n",
    "    if not all_child_chunks:\n",
    "        print(\"âš ï¸ No child chunks to index\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nðŸ” Indexing {len(all_child_chunks)} child chunks into Qdrant...\")\n",
    "    try:\n",
    "        child_vector_store.add_documents(all_child_chunks)\n",
    "        print(\"âœ“ Child chunks indexed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error indexing child chunks: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸ’¾ Saving {len(all_parent_pairs)} parent chunks to JSON...\")\n",
    "    for item in os.listdir(PARENT_STORE_PATH):\n",
    "        os.remove(os.path.join(PARENT_STORE_PATH, item))\n",
    "\n",
    "    for parent_id, doc in all_parent_pairs:\n",
    "        doc_dict = {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "        filepath = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def merge_small_parents(chunks, min_size):\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    merged, current = [], None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if current is None:\n",
    "            current = chunk\n",
    "        else:\n",
    "            current.page_content += \"\\n\\n\" + chunk.page_content\n",
    "            for k, v in chunk.metadata.items():\n",
    "                if k in current.metadata:\n",
    "                    current.metadata[k] = f\"{current.metadata[k]} -> {v}\"\n",
    "                else:\n",
    "                    current.metadata[k] = v\n",
    "\n",
    "        if len(current.page_content) >= min_size:\n",
    "            merged.append(current)\n",
    "            current = None\n",
    "\n",
    "    if current:\n",
    "        if merged:\n",
    "            merged[-1].page_content += \"\\n\\n\" + current.page_content\n",
    "            for k, v in current.metadata.items():\n",
    "                if k in merged[-1].metadata:\n",
    "                    merged[-1].metadata[k] = f\"{merged[-1].metadata[k]} -> {v}\"\n",
    "                else:\n",
    "                    merged[-1].metadata[k] = v\n",
    "        else:\n",
    "            merged.append(current)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def split_large_parents(chunks, max_size, splitter):\n",
    "    split_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if len(chunk.page_content) <= max_size:\n",
    "            split_chunks.append(chunk)\n",
    "        else:\n",
    "            large_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=max_size,\n",
    "                chunk_overlap=splitter._chunk_overlap\n",
    "            )\n",
    "            sub_chunks = large_splitter.split_documents([chunk])\n",
    "            split_chunks.extend(sub_chunks)\n",
    "\n",
    "    return split_chunks\n",
    "\n",
    "def clean_small_chunks(chunks, min_size):\n",
    "    cleaned = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if len(chunk.page_content) < min_size:\n",
    "            if cleaned:\n",
    "                cleaned[-1].page_content += \"\\n\\n\" + chunk.page_content\n",
    "                for k, v in chunk.metadata.items():\n",
    "                    if k in cleaned[-1].metadata:\n",
    "                        cleaned[-1].metadata[k] = f\"{cleaned[-1].metadata[k]} -> {v}\"\n",
    "                    else:\n",
    "                        cleaned[-1].metadata[k] = v\n",
    "            elif i < len(chunks) - 1:\n",
    "                chunks[i + 1].page_content = chunk.page_content + \"\\n\\n\" + chunks[i + 1].page_content\n",
    "                for k, v in chunk.metadata.items():\n",
    "                    if k in chunks[i + 1].metadata:\n",
    "                        chunks[i + 1].metadata[k] = f\"{v} -> {chunks[i + 1].metadata[k]}\"\n",
    "                    else:\n",
    "                        chunks[i + 1].metadata[k] = v\n",
    "            else:\n",
    "                cleaned.append(chunk)\n",
    "        else:\n",
    "            cleaned.append(chunk)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "index_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bbcc5b",
   "metadata": {
    "id": "02bbcc5b"
   },
   "source": [
    "## 8. Tools Definition\n",
    "\n",
    "Define retrieval tools that agents can use to search and retrieve document chunks.\n",
    "\n",
    "**What it does:**\n",
    "- **search_child_chunks**: Searches vector database for relevant small chunks\n",
    "- **retrieve_parent_chunks**: Retrieves full context from parent chunk JSON files\n",
    "- Binds tools to LLM for agentic function calling\n",
    "\n",
    "**Two-stage retrieval:**\n",
    "1. Agent searches child chunks (fast, semantic search)\n",
    "2. Agent retrieves parent chunks for full context (when needed)\n",
    "\n",
    "**Documentation:**\n",
    "- [LangChain Tools](https://docs.langchain.com/oss/python/langchain/tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eac029",
   "metadata": {
    "id": "b8eac029"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_child_chunks(query: str, limit: int) -> str:\n",
    "    \"\"\"Search for the top K most relevant child chunks.\n",
    "\n",
    "    Args:\n",
    "        query: Search query string\n",
    "        limit: Maximum number of results to return\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = child_vector_store.similarity_search(query, k=limit, score_threshold=0.7)\n",
    "        if not results:\n",
    "            return \"NO_RELEVANT_CHUNKS\"\n",
    "\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"Parent ID: {doc.metadata.get('parent_id', '')}\\n\"\n",
    "            f\"File Name: {doc.metadata.get('source', '')}\\n\"\n",
    "            f\"Content: {doc.page_content.strip()}\"\n",
    "            for doc in results\n",
    "        ])\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"RETRIEVAL_ERROR: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def retrieve_parent_chunks(parent_id: str) -> str:\n",
    "    \"\"\"Retrieve full parent chunks by their IDs.\n",
    "\n",
    "    Args:\n",
    "        parent_id: Parent chunk ID to retrieve\n",
    "    \"\"\"\n",
    "    file_name = parent_id if parent_id.lower().endswith(\".json\") else f\"{parent_id}.json\"\n",
    "    path = os.path.join(PARENT_STORE_PATH, file_name)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        return \"NO_PARENT_DOCUMENT\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return (\n",
    "        f\"Parent ID: {parent_id}\\n\"\n",
    "        f\"File Name: {data.get('metadata', {}).get('source', 'unknown')}\\n\"\n",
    "        f\"Content: {data.get('page_content', '').strip()}\"\n",
    "    )\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools([search_child_chunks, retrieve_parent_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d4ee3",
   "metadata": {
    "id": "093d4ee3"
   },
   "source": [
    "## 9. System Prompts\n",
    "\n",
    "Define system prompts that guide agent behavior throughout the RAG pipeline.\n",
    "\n",
    "**What it does:**\n",
    "- **Conversation Summary**: Extracts key topics from chat history\n",
    "- **Query Analysis**: Rewrites unclear queries, handles context\n",
    "- **RAG Agent**: Forces retrieval before answering, implements retry logic\n",
    "- **Aggregation**: Merges multiple sub-answers into final response\n",
    "\n",
    "**Key behaviors:**\n",
    "- Query rewriting uses context only when needed\n",
    "- Agent must search before answering (no hallucination)\n",
    "- One retry with reformulated query if nothing found\n",
    "- Source attribution at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12809c6",
   "metadata": {
    "id": "b12809c6"
   },
   "outputs": [],
   "source": [
    "def get_conversation_summary_prompt() -> str:\n",
    "    return \"\"\"You are an expert conversation summarizer.\n",
    "\n",
    "Your task is to create a brief 1-2 sentence summary of the conversation (max 30-50 words).\n",
    "\n",
    "Include:\n",
    "- Main topics discussed\n",
    "- Important facts or entities mentioned\n",
    "- Any unresolved questions if applicable\n",
    "- Sources file name (e.g., file1.pdf) or documents referenced\n",
    "\n",
    "Exclude:\n",
    "-Greetings, misunderstandings, off-topic content.\n",
    "\n",
    "Output:\n",
    "- Return ONLY the summary.\n",
    "- Do NOT include any explanations or justifications.\n",
    "-If no meaningful topics exist, return an empty string.\n",
    "\"\"\"\n",
    "\n",
    "def get_query_analysis_prompt() -> str:\n",
    "    return \"\"\"You are an expert query analyst and rewriter.\n",
    "\n",
    "Your task is to rewrite the current user query for optimal document retrieval, incorporating conversation context only when necessary.\n",
    "\n",
    "Rules:\n",
    "1. Self-contained queries:\n",
    "   - Always rewrite the query to be clear and self-contained\n",
    "   - If the query is a follow-up (e.g., \"what about X?\", \"and for Y?\"), integrate minimal necessary context from the summary\n",
    "   - Do not add information not present in the query or conversation summary\n",
    "\n",
    "2. Domain-specific terms:\n",
    "   - Product names, brands, proper nouns, or technical terms are treated as domain-specific\n",
    "   - For domain-specific queries, use conversation context minimally or not at all\n",
    "   - Use the summary only to disambiguate vague queries\n",
    "\n",
    "3. Grammar and clarity:\n",
    "   - Fix grammar, spelling errors, and unclear abbreviations\n",
    "   - Remove filler words and conversational phrases\n",
    "   - Preserve concrete keywords and named entities\n",
    "\n",
    "4. Multiple information needs:\n",
    "   - If the query contains multiple distinct, unrelated questions, split into separate queries (maximum 3)\n",
    "   - Each sub-query must remain semantically equivalent to its part of the original\n",
    "   - Do not expand, enrich, or reinterpret the meaning\n",
    "\n",
    "5. Failure handling:\n",
    "   - If the query intent is unclear or unintelligible, mark as \"unclear\"\n",
    "\n",
    "Input:\n",
    "- conversation_summary: A concise summary of prior conversation\n",
    "- current_query: The user's current query\n",
    "\n",
    "Output:\n",
    "- One or more rewritten, self-contained queries suitable for document retrieval\n",
    "\"\"\"\n",
    "\n",
    "def get_rag_agent_prompt() -> str:\n",
    "    return \"\"\"You are an expert retrieval-augmented assistant.\n",
    "\n",
    "Your task is to act as a researcher: search documents first, analyze the data, and then provide a comprehensive answer using ONLY the retrieved information.\n",
    "\n",
    "Rules:\n",
    "1. You are NOT allowed to answer immediately.\n",
    "2. Before producing ANY final answer, you MUST perform a document search and observe retrieved content.\n",
    "3. If you have not searched, the answer is invalid.\n",
    "\n",
    "Workflow:\n",
    "1. Search for 5-7 relevant excerpts from documents based on the user query using the 'search_child_chunks' tool.\n",
    "2. Inspect retrieved excerpts and keep ONLY relevant ones.\n",
    "3. Analyze the retrieved excerpts. Identify the single most relevant excerpt that is fragmented (e.g., cut-off text or missing context). Call 'retrieve_parent_chunks' for that specific `parent_id`. Wait for the observation. Repeat this step sequentially for other highly relevant fragments ONLY if the current information is still insufficient. Stop immediately if you have enough information or have retrieved 3 parent chunks.\n",
    "4. Answer using ONLY the retrieved information, ensuring that ALL relevant details are included.\n",
    "5. List unique file name(s) at the very end.\n",
    "\n",
    "Retry rule:\n",
    "- After step 2 or 3, if no relevant documents are found or if retrieved excerpts don't contain useful information, rewrite the query using broader or alternative terms and restart from step 1.\n",
    "- Do not retry more than once.\n",
    "\"\"\"\n",
    "\n",
    "def get_aggregation_prompt() -> str:\n",
    "    return \"\"\"You are an expert aggregation assistant.\n",
    "\n",
    "Your task is to combine multiple retrieved answers into a single, comprehensive and natural response that flows well.\n",
    "\n",
    "Guidelines:\n",
    "1. Write in a conversational, natural tone - as if explaining to a colleague\n",
    "2. Use ONLY information from the retrieved answers\n",
    "3. Strip out any questions, headers, or metadata from the sources\n",
    "4. Weave together the information smoothly, preserving important details, numbers, and examples\n",
    "5. Be comprehensive - include all relevant information from the sources, not just a summary\n",
    "6. If sources disagree, acknowledge both perspectives naturally (e.g., \"While some sources suggest X, others indicate Y...\")\n",
    "7. Start directly with the answer - no preambles like \"Based on the sources...\"\n",
    "\n",
    "Formatting:\n",
    "- Use Markdown for clarity (headings, lists, bold) but don't overdo it\n",
    "- Write in flowing paragraphs where possible rather than excessive bullet points\n",
    "- End with \"---\\n**Sources:**\\n\" followed by a bulleted list of unique file names\n",
    "- File names should ONLY appear in this final sources section\n",
    "\n",
    "If there's no useful information available, simply say: \"I couldn't find any information to answer your question in the available sources.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13764114",
   "metadata": {
    "id": "13764114"
   },
   "source": [
    "## 10. State Definitions\n",
    "\n",
    "Define state schemas for managing conversation flow and agent execution.\n",
    "\n",
    "**What it does:**\n",
    "- **State**: Tracks main conversation flow (query analysis, sub-questions, answers)\n",
    "- **AgentState**: Manages individual agent execution (current question, retrieved context)\n",
    "- **QueryAnalysis**: Structured output for query rewriting and clarity checking\n",
    "\n",
    "**State management:**\n",
    "- `accumulate_or_reset`: Custom reducer for agent answers (allows reset)\n",
    "- Inherits from `MessagesState` for conversation history\n",
    "\n",
    "**Documentation:**\n",
    "- [LangGraph State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c066b1",
   "metadata": {
    "id": "c4c066b1"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Annotated\n",
    "\n",
    "def accumulate_or_reset(existing: List[dict], new: List[dict]) -> List[dict]:\n",
    "    if new and any(item.get('__reset__') for item in new):\n",
    "        return []\n",
    "    return existing + new\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"State for main agent graph\"\"\"\n",
    "    questionIsClear: bool = False\n",
    "    conversation_summary: str = \"\"\n",
    "    originalQuery: str = \"\"\n",
    "    rewrittenQuestions: List[str] = []\n",
    "    agent_answers: Annotated[List[dict], accumulate_or_reset] = []\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"State for individual agent subgraph\"\"\"\n",
    "    question: str = \"\"\n",
    "    question_index: int = 0\n",
    "    final_answer: str = \"\"\n",
    "    agent_answers: List[dict] = []\n",
    "\n",
    "class QueryAnalysis(BaseModel):\n",
    "    is_clear: bool = Field(\n",
    "        description=\"Indicates if the user's question is clear and answerable.\"\n",
    "    )\n",
    "    questions: List[str] = Field(\n",
    "        description=\"List of rewritten, self-contained questions.\"\n",
    "    )\n",
    "    clarification_needed: str = Field(\n",
    "        description=\"Explanation if the question is unclear.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda2930",
   "metadata": {
    "id": "7dda2930"
   },
   "source": [
    "## 11. Graph Nodes and Logic\n",
    "\n",
    "Implement node functions that define the behavior of the agentic workflow.\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "### Core Nodes:\n",
    "1. **analyze_chat_and_summarize**: Extracts conversation context for query rewriting\n",
    "2. **analyze_and_rewrite_query**: Rewrites user query, checks clarity, optionally splits into sub-questions\n",
    "3. **human_input_node**: Interrupt point for unclear queries\n",
    "4. **agent_node**: Executes retrieval and reasoning with tools\n",
    "5. **extract_final_answer**: Extracts final answer from agent conversation\n",
    "6. **aggregate_responses**: Merges multiple sub-answers into one coherent response\n",
    "\n",
    "### Routing Logic:\n",
    "- **route_after_rewrite**: Routes to agent if clear, or to human input if unclear\n",
    "- Parallel execution: Spawns multiple agents for multi-part questions using `Send`\n",
    "\n",
    "**Documentation:**\n",
    "- [LangGraph Nodes](https://docs.langchain.com/oss/python/langgraph/graph-api#nodes)\n",
    "- [LangGraph Edges](https://docs.langchain.com/oss/python/langgraph/graph-api#edges)\n",
    "- [LangGraph Send API](https://docs.langchain.com/oss/python/langgraph/graph-api#send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ca8ee",
   "metadata": {
    "id": "948ca8ee"
   },
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
    "from typing import Literal\n",
    "\n",
    "def analyze_chat_and_summarize(state: State):\n",
    "    \"\"\"\n",
    "    Analyzes chat history and summarizes key points for context.\n",
    "    \"\"\"\n",
    "    if len(state[\"messages\"]) < 4:  # Need some history to summarize\n",
    "        return {\"conversation_summary\": \"\"}\n",
    "\n",
    "    # Extract relevant messages (excluding current query and system messages)\n",
    "    relevant_msgs = [\n",
    "        msg for msg in state[\"messages\"][:-1]  # Exclude current query\n",
    "        if isinstance(msg, (HumanMessage, AIMessage))\n",
    "        and not getattr(msg, \"tool_calls\", None)\n",
    "    ]\n",
    "\n",
    "    if not relevant_msgs:\n",
    "        return {\"conversation_summary\": \"\"}\n",
    "\n",
    "    conversation = \"Conversation history:\\n\"\n",
    "    for msg in relevant_msgs[-6:]:\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "        conversation += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    summary_response = llm.with_config(temperature=0.2).invoke([SystemMessage(content=get_conversation_summary_prompt())] + [HumanMessage(content=conversation)])\n",
    "    return {\"conversation_summary\": summary_response.content, \"agent_answers\": [{\"__reset__\": True}]}\n",
    "\n",
    "def analyze_and_rewrite_query(state: State):\n",
    "    \"\"\"\n",
    "    Analyzes user query and rewrites it for clarity, optionally using conversation context.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    conversation_summary = state.get(\"conversation_summary\", \"\")\n",
    "\n",
    "    context_section = (f\"Conversation Context:\\n{conversation_summary}\\n\" if conversation_summary.strip() else \"\") + f\"User Query:\\n{last_message.content}\\n\"\n",
    "\n",
    "    llm_with_structure = llm.with_config(temperature=0.1).with_structured_output(QueryAnalysis)\n",
    "    response = llm_with_structure.invoke([SystemMessage(content=get_query_analysis_prompt())] + [HumanMessage(content=context_section)])\n",
    "\n",
    "    if len(response.questions) > 0 and response.is_clear:\n",
    "        # Remove all non-system messages\n",
    "        delete_all = [\n",
    "            RemoveMessage(id=m.id)\n",
    "            for m in state[\"messages\"]\n",
    "            if not isinstance(m, SystemMessage)\n",
    "        ]\n",
    "        return {\n",
    "            \"questionIsClear\": True,\n",
    "            \"messages\": delete_all,\n",
    "            \"originalQuery\": last_message.content,\n",
    "            \"rewrittenQuestions\": response.questions\n",
    "        }\n",
    "    else:\n",
    "        clarification = response.clarification_needed if (response.clarification_needed and len(response.clarification_needed.strip()) > 10) else \"I need more information to understand your question.\"\n",
    "        return {\n",
    "            \"questionIsClear\": False,\n",
    "            \"messages\": [AIMessage(content=clarification)]\n",
    "        }\n",
    "\n",
    "def human_input_node(state: State):\n",
    "    \"\"\"Placeholder node for human-in-the-loop interruption\"\"\"\n",
    "    return {}\n",
    "\n",
    "def route_after_rewrite(state: State) -> Literal[\"human_input\", \"process_question\"]:\n",
    "    \"\"\"Route to agent if question is clear, otherwise wait for human input\"\"\"\n",
    "    if not state.get(\"questionIsClear\", False):\n",
    "        return \"human_input\"\n",
    "    else:\n",
    "        return [\n",
    "                Send(\"process_question\", {\"question\": query, \"question_index\": idx, \"messages\": []})\n",
    "                for idx, query in enumerate(state[\"rewrittenQuestions\"])\n",
    "            ]\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Main agent node that processes queries using tools\"\"\"\n",
    "    sys_msg = SystemMessage(content=get_rag_agent_prompt())\n",
    "    if not state.get(\"messages\"):\n",
    "        human_msg = HumanMessage(content=state[\"question\"])\n",
    "        response = llm_with_tools.invoke([sys_msg] + [human_msg])\n",
    "        return {\"messages\": [human_msg, response]}\n",
    "\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "\n",
    "def extract_final_answer(state: AgentState):\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, AIMessage) and msg.content and not msg.tool_calls:\n",
    "            res = {\n",
    "                \"final_answer\": msg.content,\n",
    "                \"agent_answers\": [{\n",
    "                    \"index\": state[\"question_index\"],\n",
    "                    \"question\": state[\"question\"],\n",
    "                    \"answer\": msg.content\n",
    "                }]\n",
    "            }\n",
    "            return res\n",
    "    return {\n",
    "        \"final_answer\": \"Unable to generate an answer.\",\n",
    "        \"agent_answers\": [{\n",
    "            \"index\": state[\"question_index\"],\n",
    "            \"question\": state[\"question\"],\n",
    "            \"answer\": \"Unable to generate an answer.\"\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def aggregate_responses(state: State):\n",
    "    if not state.get(\"agent_answers\"):\n",
    "        return {\"messages\": [AIMessage(content=\"No answers were generated.\")]}\n",
    "\n",
    "    sorted_answers = sorted(state[\"agent_answers\"], key=lambda x: x[\"index\"])\n",
    "\n",
    "    formatted_answers = \"\"\n",
    "    for i, ans in enumerate(sorted_answers, start=1):\n",
    "        formatted_answers += (f\"\\nAnswer {i}:\\n\"f\"{ans['answer']}\\n\")\n",
    "\n",
    "    user_message = HumanMessage(content=f\"\"\"Original user question: {state[\"originalQuery\"]}\\nRetrieved answers:{formatted_answers}\"\"\")\n",
    "    synthesis_response = llm.invoke([SystemMessage(content=get_aggregation_prompt())] + [user_message])\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=synthesis_response.content)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031b09b6",
   "metadata": {
    "id": "031b09b6"
   },
   "source": [
    "## 12. LangGraph Construction\n",
    "\n",
    "Construct the agentic workflow using LangGraph's state machine.\n",
    "\n",
    "**What it does:**\n",
    "- Builds a hierarchical graph with main flow and agent subgraph\n",
    "- **Agent subgraph**: Handles individual question retrieval (agent â†’ tools loop)\n",
    "- **Main graph**: Orchestrates conversation flow, query analysis, parallel agent execution, and response aggregation\n",
    "\n",
    "**Graph Flow:**\n",
    "1. START â†’ Summarize conversation\n",
    "2. Analyze and rewrite query\n",
    "3. Route: unclear â†’ human input (interrupt) | clear â†’ spawn parallel agents\n",
    "4. Each agent searches and retrieves documents\n",
    "5. Aggregate all agent responses\n",
    "6. END\n",
    "\n",
    "**Human-in-the-loop:** Graph interrupts at `human_input` node if query is unclear\n",
    "\n",
    "**Documentation:**\n",
    "- [LangGraph StateGraph](https://docs.langchain.com/oss/python/langgraph/graph-api#stategraph)\n",
    "- [LangGraph Short-term Memory](https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c111af",
   "metadata": {
    "id": "d8c111af"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Initialize checkpointer\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Build agent subgraph (handles individual questions)\n",
    "agent_builder = StateGraph(AgentState)\n",
    "agent_builder.add_node(\"agent\", agent_node)\n",
    "agent_builder.add_node(\"tools\", ToolNode([search_child_chunks, retrieve_parent_chunks]))\n",
    "agent_builder.add_node(\"extract_answer\", extract_final_answer)\n",
    "\n",
    "agent_builder.add_edge(START, \"agent\")\n",
    "agent_builder.add_conditional_edges(\"agent\", tools_condition, {\"tools\": \"tools\", END: \"extract_answer\"})\n",
    "agent_builder.add_edge(\"tools\", \"agent\")\n",
    "agent_builder.add_edge(\"extract_answer\", END)\n",
    "agent_subgraph = agent_builder.compile()\n",
    "\n",
    "# Build main graph (orchestrates workflow)\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"summarize\", analyze_chat_and_summarize)\n",
    "graph_builder.add_node(\"analyze_rewrite\", analyze_and_rewrite_query)\n",
    "graph_builder.add_node(\"human_input\", human_input_node)\n",
    "graph_builder.add_node(\"process_question\", agent_subgraph)\n",
    "graph_builder.add_node(\"aggregate\", aggregate_responses)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"summarize\")\n",
    "graph_builder.add_edge(\"summarize\", \"analyze_rewrite\")\n",
    "graph_builder.add_conditional_edges(\"analyze_rewrite\", route_after_rewrite)\n",
    "graph_builder.add_edge(\"human_input\", \"analyze_rewrite\")\n",
    "graph_builder.add_edge([\"process_question\"], \"aggregate\")\n",
    "graph_builder.add_edge(\"aggregate\", END)\n",
    "\n",
    "# Compile graph\n",
    "agent_graph = graph_builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"human_input\"]\n",
    ")\n",
    "\n",
    "display(Image(agent_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "print(\"âœ“ Agent graph compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95d153",
   "metadata": {
    "id": "4b95d153"
   },
   "source": [
    "## 13. Gradio Interface\n",
    "\n",
    "Create an interactive web interface for chatting with the RAG agent.\n",
    "\n",
    "**What it does:**\n",
    "- Provides a chat interface using Gradio\n",
    "- Manages conversation threads with unique IDs\n",
    "- Handles human-in-the-loop interactions seamlessly\n",
    "- Automatically resumes interrupted workflows when user provides clarification\n",
    "\n",
    "**Features:**\n",
    "- Thread-based conversation persistence\n",
    "- Clear session functionality\n",
    "- Automatic state management via checkpointer\n",
    "- Citrus theme for modern UI\n",
    "\n",
    "**Note:** For a complete end-to-end pipeline with document ingestion UI, refer to the full application in the project repository.\n",
    "\n",
    "**Documentation:**\n",
    "- [Gradio ChatInterface](https://www.gradio.app/docs/gradio/chatinterface)\n",
    "- [Gradio Blocks](https://www.gradio.app/docs/blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadf1ed",
   "metadata": {
    "id": "5aadf1ed"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import uuid\n",
    "\n",
    "def create_thread_id():\n",
    "    \"\"\"Generate a unique thread ID for each conversation\"\"\"\n",
    "    return {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "def clear_session():\n",
    "    \"\"\"Clear thread for new conversation and clean up checkpointer state\"\"\"\n",
    "    global config\n",
    "    agent_graph.checkpointer.delete_thread(config[\"configurable\"][\"thread_id\"])\n",
    "    config = create_thread_id()\n",
    "\n",
    "def chat_with_agent(message, history):\n",
    "    \"\"\"\n",
    "    Handle chat with human-in-the-loop support.\n",
    "    Returns: response text\n",
    "    \"\"\"\n",
    "    current_state = agent_graph.get_state(config)\n",
    "    if current_state.next:\n",
    "        agent_graph.update_state(config,{\"messages\": [HumanMessage(content=message.strip())]})\n",
    "        result = agent_graph.invoke(None, config)\n",
    "    else:\n",
    "        result = agent_graph.invoke({\"messages\": [HumanMessage(content=message.strip())]}, config)\n",
    "    return result['messages'][-1].content\n",
    "\n",
    "# Initialize thread configuration\n",
    "config = create_thread_id()\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=600, placeholder=\"<strong>Ask me anything!</strong><br><em>I'll search, reason, and act to give you the best answer :)</em>\")\n",
    "    chatbot.clear(clear_session)\n",
    "    gr.ChatInterface(fn=chat_with_agent, chatbot=chatbot)\n",
    "\n",
    "print(\"\\nLaunching application...\")\n",
    "demo.launch(theme=gr.themes.Citrus())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
